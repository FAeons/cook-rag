import logging

from langchain_openai import ChatOpenAI 
import os
from langchain_core.documents import Document
from typing import List,Generator
from langchain_core.prompts import ChatPromptTemplate,PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class GenerationIntegrationModule:
    """ç”Ÿæˆé›†æˆæ¨¡å—: è´Ÿè´£ç”Ÿæˆå’Œé›†æˆ"""

    def __init__(self, model_name: str = "deepseek-chat",temperature: float = 0.1, max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—

        Args:
            model_name (str): ç”Ÿæˆæ¨¡å‹åç§°
            temperature (float): ç”Ÿæˆæ¸©åº¦
            max_tokens (int): ç”Ÿæˆæœ€å¤§é•¿åº¦
        """

        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = None
        self.setup_llm()

    def setup_llm(self):
        """è®¾ç½®LLM"""
        logger.info("æ­£åœ¨è®¾ç½®LLM...")

        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            error_msg = "è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡"
            logger.error(error_msg)
            raise ValueError(error_msg) 
        
        base_url = os.getenv("DEEPSEEK_BASE_URL")
        if not base_url:
            logger.error("è¯·è®¾ç½® DEEPSEEK_BASE_URL ç¯å¢ƒå˜é‡")
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_BASE_URL ç¯å¢ƒå˜é‡")

        self.llm = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            api_key=api_key,
            base_url=base_url,
        )
        logger.info(f"æˆåŠŸåŠ è½½LLM{self.model_name}")

    def generate_basic_answer(self, query: str, context_docs: List[Document]) -> str:
        """ç”ŸæˆåŸºç¡€ç­”æ¡ˆ

        Args:
            question (str): é—®é¢˜
            context_docs (List[Document]): ä¸Šä¸‹æ–‡æ–‡æ¡£

        Returns:
            str: ç­”æ¡ˆ
        """

        context = self._build_context(context_docs)
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹é£Ÿè°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")
        
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)

        return response


    def generate_step_by_step_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ†æ­¥éª¤å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ†æ­¥éª¤çš„è¯¦ç»†å›ç­”
        """        
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªå¯¼å¸ˆã€‚è¯·æ ¹æ®é£Ÿè°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æ­¥éª¤æŒ‡å¯¼ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ¥˜ èœå“ä»‹ç»
[ç®€è¦ä»‹ç»èœå“ç‰¹ç‚¹å’Œéš¾åº¦]

## ğŸ›’ æ‰€éœ€é£Ÿæ
[åˆ—å‡ºä¸»è¦é£Ÿæå’Œç”¨é‡]

## ğŸ‘¨â€ğŸ³ åˆ¶ä½œæ­¥éª¤
[è¯¦ç»†çš„åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯æ­¥åŒ…å«å…·ä½“æ“ä½œå’Œå¤§æ¦‚æ‰€éœ€æ—¶é—´]

## ğŸ’¡ åˆ¶ä½œæŠ€å·§
[ä»…åœ¨æœ‰å®ç”¨æŠ€å·§æ—¶åŒ…å«ã€‚ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä¸­çš„å®ç”¨æŠ€å·§ï¼Œå¦‚æœåŸæ–‡çš„"é™„åŠ å†…å®¹"ä¸çƒ¹é¥ªæ— å…³æˆ–ä¸ºç©ºï¼Œå¯ä»¥åŸºäºåˆ¶ä½œæ­¥éª¤æ€»ç»“å…³é”®è¦ç‚¹ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹æˆ–é‡å¤åˆ¶ä½œæ­¥éª¤ä¸­çš„ä¿¡æ¯
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§
- å¦‚æœæ²¡æœ‰é¢å¤–çš„æŠ€å·§è¦åˆ†äº«ï¼Œå¯ä»¥çœç•¥åˆ¶ä½œæŠ€å·§éƒ¨åˆ†

å›ç­”:""")
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def query_rewrite(self, query: str) -> str:
        """é‡å†™æŸ¥è¯¢ - è®©å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦è¦é‡å†™æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            é‡å†™åçš„æŸ¥è¯¢æˆ–åŸå§‹æŸ¥è¯¢
        """
        prompt = PromptTemplate(
            template="""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ä»¥æé«˜é£Ÿè°±æœç´¢æ•ˆæœã€‚

åŸå§‹æŸ¥è¯¢: {query}

åˆ†æè§„åˆ™ï¼š
1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢** (ç›´æ¥è¿”å›åŸå§‹æŸ¥è¯¢)
    - åŒ…å«å…·ä½“èœå“åç§°ï¼šå¦‚"å®«ä¿é¸¡ä¸æ€ä¹ˆåš"ï¼Œ "çº¢çƒ§è‚‰çš„åˆ¶ä½œæ–¹æ³•"
    - æ˜ç¡®çš„åˆ¶ä½œè¯¢é—®ï¼š å¦‚"è›‹ç‚’é¥­éœ€è¦ä»€ä¹ˆé£Ÿæ"ï¼Œ "ç³–é†‹æ’éª¨çš„åˆ¶ä½œæ­¥éª¤"
    - å…·ä½“çš„çƒ¹é¥ªæŠ€å·§ï¼š å¦‚"å¦‚ä½•ç‚’èœä¸ç²˜é”…"ï¼Œ "æ€æ ·è°ƒåˆ¶ç³–é†‹æ±"
2. **æ¨¡ç³Šçš„æŸ¥è¯¢** (æ ¹æ®æŸ¥è¯¢å†…å®¹è¿›è¡Œé‡å†™)
    - è¿‡äºå®½æ³›ï¼š å¦‚"åšèœ"ï¼Œ"æœ‰ä»€ä¹ˆå¥½åƒçš„"ï¼Œ"æ¨èä¸ªèœ"
    - ç¼ºä¹å…·ä½“ä¿¡æ¯ï¼š å¦‚"å·èœ"ï¼Œ"ç´ èœ"ï¼Œ"ç®€å•çš„"
    - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"æƒ³åƒä»€ä¹ˆ"ï¼Œ"æœ‰é¥®å“æ¨èå—"

é‡å†™åŸåˆ™ï¼š
- ä¿æŒåŸæ„ä¸å˜
- å¢åŠ ç›¸å…³çƒ¹é¥ªæœ¯è¯­
- ä¿æŒæ¨èç®€å•æ˜“åšçš„
- ä¿æŒç®€æ´æ€§

ç¤ºä¾‹ï¼š
- "åšèœ" â†’ "ç®€å•æ˜“åšçš„å®¶å¸¸èœè°±"
- "æœ‰é¥®å“æ¨èå—" â†’ "ç®€å•é¥®å“åˆ¶ä½œæ–¹æ³•"
- "æ¨èä¸ªèœ" â†’ "ç®€å•å®¶å¸¸èœæ¨è"
- "å·èœ" â†’ "ç»å…¸å·èœèœè°±"
- "å®«ä¿é¸¡ä¸æ€ä¹ˆåš" â†’ "å®«ä¿é¸¡ä¸æ€ä¹ˆåš" ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
- "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ" â†’ "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ" ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

è¯·è¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰:""",
            input_variables=["query"],
        )
        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()           
        )

        response = chain.invoke(query).strip()

        #è®°å½•é‡å†™ç»“æœ
        if response.strip() != query:
            logger.info(f"æŸ¥è¯¢é‡å†™ï¼š{query} â†’ {response}")
        else:
            logger.info(f"æŸ¥è¯¢æ— éœ€é‡å†™ï¼š{query}")
        return response 
    
    def query_router(self, query: str) -> str:
        """
        æŸ¥è¯¢è·¯ç”± - æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è·¯ç”±ç±»å‹ ('list', 'detail', 'general')       
        """
        prompt = ChatPromptTemplate.from_template("""
æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š

1. 'list' - ç”¨æˆ·æƒ³è¦è·å–èœå“åˆ—è¡¨æˆ–æ¨èï¼Œåªéœ€è¦èœå
    ä¾‹å¦‚ï¼š æ¨èå‡ ä¸ªç´ èœ;æœ‰ä»€ä¹ˆåƒå·èœ;ç»™æˆ‘3ä¸ªç®€å•çš„èœ


2. 'detail' - ç”¨æˆ·æƒ³è¦è·å–èœå“çš„è¯¦ç»†åˆ¶ä½œä¿¡æ¯ï¼ˆåšæ³•ã€é£Ÿæã€æ­¥éª¤ç­‰ï¼‰
    ä¾‹å¦‚ï¼š å®«ä¿é¸¡ä¸æ€ä¹ˆåš;çº¢çƒ§è‚‰çš„åˆ¶ä½œæ­¥éª¤;è›‹ç‚’é¥­éœ€è¦ä»€ä¹ˆé£Ÿæ
                                                  
3. 'general' - å…¶å®ƒä¸€èˆ¬æ€§é—®é¢˜
    ä¾‹å¦‚ï¼š ä»€ä¹ˆæ˜¯å·èœ;åˆ¶ä½œæŠ€å·§;è¥å…»ä»·å€¼

è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼š list, detailæˆ–general

ç”¨æˆ·é—®é¢˜: {query} 

åˆ†ç±»ç»“æœ:""")
        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query).strip().lower()

        if response in ["list", "detail", "general"]:
            return response   
        else:
            return "general"

    def generate_list_answer(self, query: str, context_docs: List[Document]) -> str:                   
        """
        ç”Ÿæˆåˆ—è¡¨å¼å›ç­” - é€‚ç”¨äºæ¨èç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ—è¡¨å¼å›ç­”        
        """

        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„èœè°±ã€‚"

        #æå–èœå“åç§°
        dish_names = []

        for doc in context_docs:
            dish_name = doc.metadata.get("dish_name", "æœªçŸ¥èœå“")
            if dish_name not in dish_names:
                dish_names.append(dish_name)

        #æ„å»ºç®€æ´çš„åˆ—è¡¨å›ç­”
        if len(dish_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{dish_names[0]}"
        elif len(dish_names) <= 3:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š\n" +  "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names)])
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š\n" +  "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names[:3])]) + f"\n\n...è¿˜æœ‰{len(dish_names)-3}ä¸ªèœå“å¯ä¾›é€‰æ‹©ã€‚"

    def generate_basic_stream(self, query: str, context_docs: List[Document]) -> Generator[str, None, None]:
        """
        ç”ŸæˆåŸºç¡€æµå¼å›ç­” - é€‚ç”¨äºä¸€èˆ¬æ€§æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åŸºç¡€æµå¼å›ç­”        
        """
        context = self._build_context(context_docs)
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹é£Ÿè°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def generate_step_by_step_answer_stream(self,query: str, context_docs: List[Document]) -> Generator[str, None, None]:
        """
        ç”Ÿæˆæ­¥éª¤å¼å›ç­” - é€‚ç”¨äºåˆ¶ä½œæ­¥éª¤ç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            æ­¥éª¤å¼å›ç­”        
        """
        context = self._build_context(context_docs)
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªå¯¼å¸ˆã€‚è¯·æ ¹æ®é£Ÿè°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æ­¥éª¤æŒ‡å¯¼ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ¥˜ èœå“ä»‹ç»
[ç®€è¦ä»‹ç»èœå“ç‰¹ç‚¹å’Œéš¾åº¦]

## ğŸ›’ æ‰€éœ€é£Ÿæ
[åˆ—å‡ºä¸»è¦é£Ÿæå’Œç”¨é‡]

## ğŸ‘¨â€ğŸ³ åˆ¶ä½œæ­¥éª¤
[è¯¦ç»†çš„åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯æ­¥åŒ…å«å…·ä½“æ“ä½œå’Œå¤§æ¦‚æ‰€éœ€æ—¶é—´]

## ğŸ’¡ åˆ¶ä½œæŠ€å·§
[ä»…åœ¨æœ‰å®ç”¨æŠ€å·§æ—¶åŒ…å«ã€‚å¦‚æœåŸæ–‡çš„"é™„åŠ å†…å®¹"ä¸çƒ¹é¥ªæ— å…³æˆ–ä¸ºç©ºï¼Œå¯ä»¥åŸºäºåˆ¶ä½œæ­¥éª¤æ€»ç»“å…³é”®è¦ç‚¹ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def _build_context(self, docs: List[Document], max_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """

        if not docs: 
            return "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„èœè°±ã€‚"

        context_parts = []
        current_length = 0

        for i, doc in enumerate(docs, 1):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = f"ã€é£Ÿè°± {i}ã€‘"
            if 'dish_name' in doc.metadata:
                metadata_info += f" {doc.metadata['dish_name']}"
            if 'category' in doc.metadata:
                metadata_info += f" | åˆ†ç±»: {doc.metadata['category']}"
            if 'difficulty' in doc.metadata:
                metadata_info += f" | éš¾åº¦: {doc.metadata['difficulty']}"

            #æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{metadata_info}\n{doc.page_content}\n"

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n" + "="*50 + "\n" + "\n".join(context_parts)
    


if __name__ == "__main__":
    """
    ç”¨äºæµ‹è¯•ï¼š
    1. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®
    2. LLM æ˜¯å¦èƒ½æˆåŠŸè¿æ¥
    3. æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿”å›å†…å®¹
    """

    logging.basicConfig(level=logging.INFO)

    try:
        print("ğŸš€ å¼€å§‹æµ‹è¯• LLM è¿æ¥...")

        gen = GenerationIntegrationModule(
            model_name="deepseek-chat",
            temperature=0.1,
            max_tokens=200
        )

        test_query = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»å®«ä¿é¸¡ä¸"

        print("ğŸ“¨ å‘é€æµ‹è¯•è¯·æ±‚ï¼š", test_query)

        response = gen.llm.invoke(test_query)

        print("\nâœ… LLM è¿æ¥æˆåŠŸï¼è¿”å›ç»“æœå¦‚ä¸‹ï¼š\n")
        print(response.content)

    except Exception as e:
        print("\nâŒ LLM è¿æ¥å¤±è´¥ï¼é”™è¯¯ä¿¡æ¯ï¼š")
        print(e)

